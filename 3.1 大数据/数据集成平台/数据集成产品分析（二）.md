## 01 DataWorks-数据集成

DataWorks-数据集成提供了离线集成和实时集成两种方案，同时提供了面向不同业务场景的同步任务配置化方案，支持不同数据源的一键同步功能，进行简单的配置，就可以完成一个复杂业务场景。



![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TPTvibibADXibD0Bt8WaQR8uneMGMOfqnz887qe5FPtibtxALo5q2qRZV8Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

  https://www.processon.com/view/link/6224c1db6376895389223dce

### 1.1 离线集成

#### 1）基本介绍

离线（批量）的数据通道通过定义数据来源和去向的数据源和数据集，提供**一套抽象化的数据抽取插件（Reader）、数据写入插件（Writer）**，并基于此框架设计一套简化版的中间数据传输格式，从而实现任意结构化、半结构化数据源之间数据传输。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TlzcjSicJQWrgXMPl8D5icQRibHQWjzZ7XU0We7c3usEEDHCkNd8H7u8Jg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

离线集成支持通过向导模式和脚本模式来实现批量数据同步。支持对单表、文件、分库分表、整库（所有表/部分表）进行全量和增量迁移。

在增量同步中，主要通过where（RDBMS）、query（MongoDB）等子条件抽取源端增量数据（指定时间区间），并配合调度参数写入目标端对应的日期分区中，实现增量同步（追加、覆盖/更新）。

对于单表、多表（分库分表）迁移到单目标表时，支持全量和增量方式进行迁移（源表和目标表中的字段建立映射关系）。

对于整库（所有表/部分表）迁移到目标源时，考虑到历史数据量较大，提供了周期性全量（分批上传）、一次性全量、周期性增量、一次性增量、一次性全量周期性增量5种迁移方案（源表与目标表通过表名建立映射关系，目标端没有表时，支持自动建表）。

#### **2）特点/优势**

**a.支持丰富的数据源**

DataWorks-数据集成已经商业化，并且开源到社区中。因此，①支持的数据源非常丰富，②插件化

- 离线支持50+种数据源，实时支持10+数据源
- 读写任意组合（任意两种数据源组合搭配）
- 涵盖关系型数据库、MPP，NoSQL、文件存储、消息流等各大种类

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4T1PRT2bHkU561m6YZZX9BAPFvQjxCLGLO5vghGcRSvcpnYibPzseJ4Fg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**b.灵活的配置**

①支持向导模式和脚本模式配置数据同步任务，更方便和灵活，

- 向导模式对业务新手来说，上手门槛低
- 脚本模式可以提供更丰富灵活的能力，自定义配置同步条件，实现精细化的配置管理

比如，在某些业务场景中，where配置项不足以描述所筛选的条件，脚本模式下支持使用querySql来自定义筛选SQL，比如需要进行多表join后同步数据。

②数据同步任务场景化，基于不同的场景提供配置好的同步方案

**c.高效的同步速率**

离线同步中，dataworks通过下述几种方式控制/提高同步速率

①切分键

将数据表中某一列作为切分键（建议使用主键或有索引的列作为切分键），读取数据时，根据配置的字段进行数据切片，实现并发读取，可以提升数据同步速率

②通道配置

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4Tia6asOONia6b3aeMB2ak4vPytfnYXqdf3m9FFCK7ey3YicDf5JO8Moy4Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 任务期望最大并发数

​    数据同步任务内，可以从源并行读取或者并行写入数据存储端的最大线程数

- 同步速率

​    设置同步速率可以保护读取端数据库，以避免抽取速度过大，给源库造成太大压力

- 分布式处理能力

​    数据同步时，可以将任务分散到多台执行节点上并发执行，提高同步速率

​    该模式下，配置较大任务并发数会增加数据存储访问压力

- 目标端写入并发数

③离线数据同步任务调优

### **2. 实时集成**

#### **1）基本介绍**

数据集成的实时同步包括**实时读取、转换和写入三种基础插件**，各插件之间通过内部定义的中间数据格式进行交互。

一个实时同步任务支持多个转换插件进行数据清洗，并支持多个写入插件实现多路输出功能。同时针对某些场景，支持整库实时同步解决方案，您可以一次性实时同步多个表。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4T2YMaptmgBqoxWia5MPwmSBcdyN7aQvdjRky0Zw9TzDQJ78K0gRFYDicg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

以关系型数据库实时同步到maxcompute为例，

在实时同步过程中，源端表的变更记录实时写入MaCompute的是log表，然后会经过拆分（split）变为Delta表，最后Delta表再与Base表进行合并（Merge），最终结果会写入Base表中

在实际执行中，实时集成会根据base表自动生产log表和delta表，并且**自动生成拆分（split）与合并（merge）的离线任务**，并会自动启动他们，作为用户只需要关心最终的base表即可。

- 在写入目标目标表时，写入模式分为写入日志表和写入增量表

- - **写入日志表**表示源端所有表的变更记录写入一张maxcompute log表中，只有在“同步解决方案”里才支持此选项，因为要配合着对应的拆分任务才有意义
  - **写入增量表**表示源端表的变更记录直接写入对应的maxcompute增量表中，此模式目前暂不在“同步解决方案”可选，因为“同步解决方案”中已经可以自动建立上下游所有任务了，可以完整实现最终全量表的结果更新

- 实时同步在同步MySQL、Oracle、loghub和polarDB类型的数据至maxcompute、datahub或kafka时，会在同步的目标端（日志表/增量表）添加5个附加列，以便进行元数据管理、排序去重等操作

- 实时同步校验数据准确性通过**手动配置数据质量规则**来对比数据量的一致性

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TbUar0MYu0jIUribwqbRON5aPwu49SRM1Nga4AnMS0DWd5ljxRicRNzpg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**2）特点/优势**

- **数据源丰富多样**

​    支持星型链路组合，可以将多种输入及输出数据源搭配组成同步链路进行数据同步。

- **采用解决方案系统**

​    支持常见数据库的整库同步，实现先全量，再持续增量同步数据。

- **同步数据方式多样**

​    可以选择分库分表、单表或整库多表等多种方式进行数据同步，同时，也可以根据不同DDL消息配置不同实时同步规则。

​    可以编辑已经配置运行的同步任务，为该任务快速添加表或删除已有同步表。

- **支持数据处理**

​    可以根据业务需求，对输入数据源进行数据过滤、字符串替换和数据脱敏处理后再进行输出。

- **支持监控运维告警**

​    对于业务延迟、Failover、脏数据、心跳检查和失败信息，支持通过邮件、短信、电话和钉钉通知发送相应告警，方便及时发现并处理报错信息。

- **使用图形化开发**

​    无需编写代码，直接通过托拽的方式即可进行任务的开发。业务新手也能够轻松上手。

## 02 **DGC-数据集成**

DGC-数据集成提供离线同步和实时同步两种同步方式。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TDXeEqmoXPEwEBQtMwI06HlqUHaBibeFFNWicUMMkBNGdczibWcQMdJ7cg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

离线同步通过向导模式实现，实时同步则通过脚本命令的方式实现。

### **1.离线同步**

**1）基本介绍**

CDM数据迁移采用抽取-写入模式，CDM首先从源端数据源抽取数据，然后再将数据写入目标端数据源，数据访问操作都是有CDM主动发起。迁移过程中需要提供源端与目标端数据源的用户名和密码，存放在CDM实例的数据库中。对于数据源（如RDS数据源）支持SSL时，会使用SSL加密传输。

在创建数据迁移任务前，需要创建数据连接，让CDM能够读写数据源。一个迁移任务需要建立两个连接：源连接和目的连接。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4Tlnwv8AT8ia2fxbL4xVicvNib3AluWN9lSY4y0eOh6ThaEAdfYribMakQfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**2）操作流程**

**step1**：创建CDM集群

**step2**：创建连接

通过创建数据连接让CDM集群能够读写数据源

需要创建两个连接，一个用于CDM连接到源端数据源，一个用于CDM连接到目的端数据源

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TLVGK4RgndSZ3heF9ibUOxKhiavDvaIFOqSP8j9u6yOObfibgysAenFZ6Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**step3**：创建同步作业

- 输入作业名称
- 选择源连接、目标连接
- 配置源端与目标端作业参数

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TelDYo9tNsV1dqHwdSG6a6SfA9gQJmh0UcFJojMCyXc9USRp5UNItMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**step4**：字段映射

CDM会自动匹配源端和目标端数据表字段，需要用户检查目的端的数据是否完整（支持添加新字段方式补齐）、字段映射关系和时间格式是否正确（支持拖拽方式调整映射关系），比如源字段类型是否可以转换为目标字段类型

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4T35lYQVKlrdIibo6oJxSe72DGDzxRQlSibYm0gmg7sBJ9Xicrua6YTE3Ww/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**step5**：配置任务参数

任务参数配置包括是否定时任务、作业失败是否重试、是否写入脏数据等

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TCMqtO2l7ic1wxPYg8AHsuusbtto1XBOaicdGZ4VSZnh8TqCwQ0Yoico8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**step6**：保存并运行作业

**3）产品功能**

- **表/文件/整库迁移**

支持批量迁移表或者文件，还支持同构/异构数据库之间整库迁移，一个作业即可迁移几百张表。

- - 在数据库迁移场景下，一个作业等价于迁移一张表，在文件迁移场景下，一个作业可以迁移多个文件

- **增量数据迁移**

支持文件增量迁移、关系型数据库增量迁移、HBase/CloudTable增量迁移，以及使用Where条件配合时间变量函数实现增量数据迁移。

- **事务模式迁移**

支持当CDM作业执行失败时，将数据回滚到作业开始之前的状态，自动清理目的表中的数据。

- - CDM会自动创建**临时表（阶段表）**，先将数据导入到该临时表，导入成功后再通过数据库的事务模式将数据迁移到目标表中，导入到临时表失败的话，则将目标表回滚到作业开始之前的状态
  - 适用于数据库离线迁移场景，仅支持有限的数据库增量迁移，不支持数据库实时增量迁移

- **字段转换**

支持去隐私、字符串操作、日期操作等常用字段的数据转换功能。

- **文件加密**

在迁移文件到文件系统时，CDM支持对写入云端的文件进行加密。

- **MD5校验一致性**

支持使用MD5校验，检查端到端文件的一致性，并输出校验结果。

- **脏数据归档**

支持将迁移过程中处理失败的、被清洗过滤掉的、不符合字段转换或者不符合清洗规则的数据单独归档到脏数据日志中，便于用户查看。并支持设置脏数据比例阈值，来决定任务是否成功。

### **2. 实时同步**

**1）基本介绍**

DGC实时数据接入服务（Data Ingestion Service）为处理或分析流数据的自定义应用程序构建数据流管道，主要解决云服务外的数据实时传输到云服务内的问题。实时数据接入每小时可从数十万种数据源（如日志和定位追踪事件、网站点击流、社交媒体源等）中连续捕获、传送和存储数TB数据。

实时集成支持的数据源：

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JKibne1Kty6XXpYVKlgUiaC4TfOe3QetvdZHI2UibTbvyLuPHNN4ia0ynnGz7GhSibm4GOmlfSzI6q8fibQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

####  **2） 操作流程**

**step1**：购买实时数据接入通道，开通DIS通道

**step2**：准备DIS应用开发环境

用户开发DIS应用程序前，首先需要安装应用开发工具。然后获取SDK和样例工程，并导入到用户的开发环境中

**step3**：发送数据到接入通道

基于数据上传业务安装应用程序，并运行程序，实现数据上传功能。数据上传过程中可在Console控制台查看数据上传通道相关信息

**step4**：从DIS获取数据

基于删除下载业务开发应用程序，并运行程序，实现数据下载功能