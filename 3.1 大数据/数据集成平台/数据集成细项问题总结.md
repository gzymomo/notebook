- [数据集成细项问题总结 (qq.com)](https://mp.weixin.qq.com/s/7VcD4w6ZdNff66Le-0-gCQ)

## 1. 映射关系

要**保证数据从源端迁移到目标端对应位置中**（若将表比作教室，字段就是座位，数据（就是学生）必须对号入座），需要在源与目标之间建立映射关系，映射关系主要包括两方面：表（库）映射、字段映射。

- 表（库）映射：表与表之间的映射。映射规则是多样的，比如源数据写入与源表**表名相同**的目标表中、源数据写入表名经过一定的转换规则的目标表中

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblmhsck9vVicNL5hUibYuDEebE7QhjFcBKjlqQ6XSXoZoYHOxTvOSemgicw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（DataWorks提供的表（库）映射规则）

- 字段映射：字段与字段之间的映射，包括两方面：字段名与字段名的映射，字段类型与字段类型的转换（转换规则包括数据类型、null值处理、字符串脱敏/替换等）

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblDmCFEicH9PJibjf3I6oMuaCjd7ic1kd9IyKoX8sQABXAOyCMUALgSamHQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（DGC提供的不同源端数据源迁移到DWS数据源时，数据类型的转换规则）

当源端数据迁移到目标端时，可能存在目标端还不存在对应的表，所以数据集成一般都会提供**建表**功能，在建表时，需要按照表和字段的映射规则进行创建，这样才能使数据落到正确的位置。

## 2. 数据同步的内容

哲学上有个命题：我是谁？我从哪来？我要到哪去？。类比到数据同步场景，表（库）/字段的映射关系用于说明：数据是从哪来的，将到哪里去。而数据同步的内容则指出了数据是什么：**表中的一行行记录**（以下将其称为实际数据）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblJn9ywIibJzAK6jauRhB1mjwtHkwvuB0kRI6w1Z2HSVFETsCKnfibsTSQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

数据同步方式受同步工具和数据时效性（离线数据和实时数据）影响，

①使用DataX对离线数据进行同步，通过在reader中配置Table、Column、Where或querySQL的信息来指定要迁移的数据，Reader插件将其拼接为SQL语句发送到Mysql数据库，脚本如下：

```json

{
    "job": {
        "setting": {
            "speed": {
                 "channel": 3
            },
            "errorLimit": {
                "record": 0,
                "percentage": 0.02
            }
        },
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": "root",
                        "password": "root",
                        "column": [
                            "id",
                            "name"
                        ],
                        "splitPk": "db_id",
                        "connection": [
                            {
                                "table": [
                                    "table"
                                ],
                                "jdbcUrl": [
     "jdbc:mysql://127.0.0.1:3306/database"
                                ]
                            }
                        ]
                    }
                },
               "writer": {
                    "name": "streamwriter",
                    "parameter": {
                        "print":true
                    }
                }
            }
        ]
    }
}
```

DataX MysqlReader插件介绍：https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md

DataX MysqlWriter插件介绍：https://github.com/alibaba/DataX/blob/master/mysqlwriter/doc/mysqlwriter.md

DataX ODPSReader插件介绍：https://github.com/alibaba/DataX/blob/master/odpsreader/doc/odpsreader.md

DataX ODPSWriter插件介绍：https://github.com/alibaba/DataX/blob/master/odpswriter/doc/odpswriter.md

DataX 介绍：https://github.com/alibaba/DataX

②使用Sqoop对离线数据进行同步，通过**SELECT SQL语句**来指定要迁移的数据，具体脚本如下：

```sql

sqoop import         
--connect jdbc:postgresql://192.168.201.75:9950/mem_center?connectTimeout=300         
--username admin         
--password ********         
--delete-target-dir         
--target-dir /user/hive/warehouse/t1002_mem_center.db/mem_member_account/dt=20220408_tmp         
--fields-terminated-by '\0x01'         
--hive-delims-replacement ' '         
--null-string '\\N'         
--null-non-string '\\N'         
--direct         
-m 6         
--query "
   SELECT 
        CAST(EXTRACT(EPOCH FROM COALESCE(update_time,create_time)::TIMESTAMP WITH TIME ZONE)*1000 AS int8) || '|9' AS event_timestamp,
        CAST('INSERT' AS VARCHAR) AS event_type, 
        T.* 
    FROM 
        mem_member_account AS T 
    WHERE 
        ((create_time >= '2022-04-08 00:00:00.000' 
            AND create_time <= '2022-04-08 23:59:59.999') 
        OR (update_time >= '2022-04-08 00:00:00.000' 
            AND update_time <= '2022-04-08 23:59:59.999')) 
        AND (1 = 0)\$CONDITIONS
    " 
--split-by create_time
```

③对于实时数据的同步，通过数据库日志解析的方式实现**数据变更记录的实时获取和实时写入，**这是和离线数据同步有很大差别的地方。当表中数据发生变化时，同时数据库日志也会产生对应的一条记录，下表为示例说明：

表

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblL2cseMoucRKicHKhXHGcaiaWibtCdxIR8xqibcKNZxWHFUicLqQMiaia11afw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

日志

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblqNhlRMnqSMdLfQQ6C85Nc9Hj7v4nC8TxDtxWTqr6Kbw5Qfic2f4Mygw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

由于数据库日志抽取是**获取所有的数据记录（增insert、改update、删delete）**，落到目标表时需要根据主键去重按照日志时间倒序排列获取最后状态的变化记录。其中对于delete类型的数据记录，有不同的处理方式，应用于不同的场景。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblnxWpibnd6JibkRjIiblUO9DxHolx3Xic0zIlkasRESOJkCjSibqfMTsujoQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（写入HDFS的实际数据）

另外，实时数据的同步比较依赖于源端数据表建设/操作的规范性，包括是否存在主键、数据变更（增改删）类型的标识等，比如当缺少主键时，写入目标表时无法区分多条数据记录是否是同一条数据的多次变更。再比如，当数据删除时日志中未给delete标识时，若源端存在删除数据的情况，在写入目标表时实际上无法判别哪条记录被删除了，这就导致源端和目标端数据的不一致。

## 3. 数据一致性校验

要**保证源端数据准确无误的迁移到目标端**，需要进行数据一致性校验，其本质是解决数据同步过程中的数据质量问题，数据一致性校验包括两个方面：**元数据校验和数据量校验**。

①元数据校验

元数据校验：校验源表与目标表的元数据信息（字段名和字段类型（基于转换规则）等）的一致性。常见的元数据不一致的情况有：源表新增/删除字段、字段名称或类型发生变更等。当元数据校验不通过时，需要基于不一致的情况对**目标表**进行处理。

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblLvpDHUz5JGVQtJrl2TqwSNviazSia7sAUibRDLmcMuTjRJcTaTVCqIiaMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（DataWorks列出了元数据（DDL类型）变更情况和对应的处理策略（由目标数据源来处理））

当元数据校验不一致时，不会影响源端数据写入目标表。原因在于：创建的目标表的表类型为**外表**。以MySQL数据同步到Hive为例，在数据同步时，真实的数据写入在HDFS的某文件路径下，目标表只是引用了路径下的数据而已（通过列分隔符、行分隔符等将实际数据与字段一一匹配）。所以当元数据不一致时，只是会导致数据写入目标表时错位等情况，对实际数据无影响。此时将目标表删了并重建或者直接修改不一致的地方，再将数据重新写入到目标表中即可。

②数据量校验

数据量校验：通过增量字段对比源表与目标表的表行数大小。数据量校验对实时数据同步来说尤为重要。当表中数据发生变化时，表中create_time和update_time会记录数据变化的时间，那么就可以通过对比一段时间范围内源端和目标端表中create_time或update_time字段值的个数来进行数据量校验了。

表

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCbl1AicpXpIjxEgvPbYTAn9OwcOp6wfpqMlL6tpmogpoicrIb0ne7Uic0hbQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

```sql
--源端create_time和update_time数据量 
select
  COUNT ( distinct
  case
    when T.create_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.create_time <= TIMESTAMP '2022-04-02 20:59:59.999' then T.id
  end ) as cnt,
  COUNT ( distinct
  case
    when T.update_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.update_time <= TIMESTAMP '2022-04-02 20:59:59.999' then T.id
  end ) as unt
from
  t_s_search_history_test1 T
where
  ( T.create_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.create_time <= TIMESTAMP '2022-04-02 20:59:59.999' )
  or ( T.update_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.update_time <= TIMESTAMP '2022-04-02 20:59:59.999' )



--目标端create_time和update_time数据量
select
  COUNT ( distinct
  case
    when T.create_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.create_time <= TIMESTAMP '2022-04-02 20:59:59.999' then T.id
  end ) as cnt,
  COUNT ( distinct
  case
    when T.update_time >= TIMESTAMP '2022-04-02 00:00:00.000'
    and T.update_time <= TIMESTAMP '2022-04-02 20:59:59.999' then T.id
  end ) as unt
from
  t003_public.t_s_search_history_test1 T
where
  T.dt >= '20220402'
```

在进行数据量校验时，会遇到以下2种数据量不一致的情况：①目标端create_time数据量比源端的要多，②目标端update_time数据量比源端的要多。

- 目标端create_time数据量比源端的要多，原因在于：目标端未排除delete类型的数据变更记录（可能目标端获取不到delete标识）
- 目标端update_time数据量比源端的要多（假设对比0点-21点的数据量），原因在于：21点后，某条数据再次进行更新操作，这时该条数据的update_time值更新为再次更新操作时的时间（该值原为2022/04/02 20:55:00，现为2022/04/02 21:00:05）。此时启动数据校验，源端count（update_time数据量）时，因为我们数据校验的时间范围为4-2 00:00:00 ~ 4-2 20:59:59，所以4-2 21:00:05这条数据不包含在统计范围内了。而目标端，校验的范围也是4-2 00:00:00 ~ 4-2 20:59:59，即能够取到2022/04/02 20:55:00这条更新记录（再次更新操作的时间是2022/04/02 21:00:05，不在校验范围），这时就会导致目标端的update数据量大于源端update数据量  

当数据量校验不通过时，可以通过重新抽数方式将指定时间范围内的数据重新抽取，写入到目标端( replace into ...) 。这种方式实际上是基于离线抽数的方式实现的。

## 4. 数据合并

当需要同步的数据量非常大时，一般会采用增量同步，即只同步新变更的增量数据。将增量数据与上一周期获取的全量数据进行合并，从而得到**最新的全量数据**。

《阿里大数据之路》这本书提到了两种合并方式：①merge方式（insert+update），②全外连接（full outer join）+数据全量覆盖重新加载（insert overwrite==>写入前清空表）。

例如日调度，将当天的增量数据和前一天的全量数据做全外连接，重新加载最新的全量数据。此外如果担心数据更新错误问题，可以采用分区方式，每天保持一个最新的全量版本，保留的较短时间周期（如3~7天）。

具体来说，

①DataWorks数据集成提供的同步解决方案中就提到“增量数据实时写入，增量数据与全量数据定时自动合并写入新的全量表分区（ 新的全量表分区意思是全量表每天都有一个分区，不是指全量表是新的，而是指分区是新的 ）

![图片](https://mmbiz.qpic.cn/mmbiz_png/bKFaqopS8JLzuBolnMuCEqPzMHzcfCblIPorDUtPvZC5tXCSfuGHGrpAdYoWMdz6wsaCLla8wRVqZUHCVib57bg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

②实时集成产品采用merge方式：源端数据先写入stg增量表中，晚上定时将增量表中的数据写入到ods中（参考ods表建表语句，实际数据应该写入到对应日期和库表的文件路径下），这样就实现与ods历史数据的合并，ods表数据为最新的全量表数据。

合并后的全量表，其数据仍然是离线的，stg增量表中的数据是实时的。在实际场景中，将离线数据与实时数据两者组合起来，

- 若既需要历史数据又需要实时数据，则可通过视图引用stg增量表+ods T+1表两部分数据；
- 若只需要实时数据，则可通过视图引用stg增量表数据
- 若只需要离线数据，则可通过视图引用ods T+1表数据