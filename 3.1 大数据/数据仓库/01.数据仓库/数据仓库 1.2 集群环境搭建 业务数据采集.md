- [数据仓库| 1.2 集群环境搭建| 业务数据采集 - kris12 - 博客园 (cnblogs.com)](https://www.cnblogs.com/shengyang17/p/14094235.html)

# 1、电商业务及数据结构

## 电商业务流程

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201207232528895-407952836.png)

电商的业务流程可以以一个普通用户的浏览足迹为例进行说明，用户点开电商首页开始浏览，可能会通过分类查询也可能通过全文搜索寻找自己中意的商品，这些商品无疑都是存储在后台的管理系统中的。

当用户寻找到自己中意的商品，可能会想要购买，将商品添加到购物车后发现需要登录，登录后对商品进行结算，这时候购物车的管理和商品订单信息的生成都会对业务数据库产生影响，会生成相应的订单数据

和支付数据。订单正式生成之后，还会对订单进行跟踪处理，直到订单全部完成。

电商的主要业务流程包括用户前台浏览商品时的商品详情的管理，用户商品加入购物车进行支付时用户个人中心&支付服务的管理，用户支付完成后订单后台服务的管理，这些流程涉及到了十几个甚至几十个业

务数据表，甚至更多。

## SKU、SPU

**SPU（Standard Product Unit），**商品信息聚合的最小单位 ，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。比如一个商品关联了其他好几个类似的商品，且这些商品很多信息如

商品图片，海报、销售属性等都是共用的； SPU表示一类商品。好处就是：可以共用商品图片，海报、销售属性等。

SKU（Stock Keeping Unit） （库存量基本单位）。现经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号，每个SKU都有自己独立的库存数。也就是说每一个商品详情展

示都是一个SKU，是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。

- 类目：类目是一个树状结构的系统，大体上可以分成4-5级。如手机->智能手机->苹果手机类目，在这里面，手机是一级类目，苹果手机是三级类目，也是叶子类目。

- SPU：苹果6（商品聚合信息的最小单位），如**手机->苹果手机->苹果6，苹果6就是SPU。**
- SKU：**土豪金 16G 苹果6 （商品的不可再分的最小单元）。**

## 电商业务表结构

本电商数仓系统涉及到的业务数据表结构关系。这24个表以订单表、用户表、SKU商品表、活动表和优惠券表为中心，延伸出了优惠券领用表、支付流水表、活动订单表、订单详情

表、订单状态表、商品评论表、编码字典表退单表、SPU商品表等，用户表提供用户的详细信息，支付流水表提供该订单的支付详情，订单详情表提供订单的商品数量等情况，商品表

给订单详情表提供商品的详细信息。本次讲解只以此24个表为例，实际项目中，业务数据库中表格远远不止这些。

![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201207232721906-385375021.png)

# 2、业务数据的生成

首先将Mysql搭建完成，创建gmall-Mysql数据库，往gmall库中导入sql结构脚本（**gmall.sql**）

### 生成业务数据

1）在hadoop101的/opt/module/目录下创建db_log文件夹

2）把gmall2020-mock-db.jar和application.properties上传到hadoop101的/opt/module/db_log路径上。

3）根据需求修改application.properties相关配置

![img](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

4）并在该目录下执行，如下命令，生成2020-06-14日期数据：

　　[kris@hadoop101 db_log]$ java -jar gmall2020-mock-db-2020-04-01.jar

5）在配置文件application.properties中修改 mock.date=2020-06-15， mock.clear=0

6）再次执行命令，生成2020-06-15日期数据：

　　[kris@hadoop101 db_log]$ java -jar gmall2020-mock-db.jar

## 业务数据建模

可借助EZDML这款数据库设计工具，来辅助我们梳理复杂的业务表关系。

**1****）下载地址**

http://www.ezdml.com/download_cn.html

# 3、业务数据采集模块

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201207231421517-2085749129.png)

## Hive&Mysql的安装

详细安装：只在1台节点hadoop101上安装即可

https://www.cnblogs.com/shengyang17/p/10372242.html

### **Hive引擎**

Hive引擎包括：默认MR、tez、spark

Hive on Spark：Hive既作为存储元数据又负责SQL的解析优化，语法是HQL语法，执行引擎变成了Spark，Spark负责采用RDD执行。

Spark on Hive : Hive只作为存储元数据，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。

**① 在Hive中配置Tez 引擎**

![img](https://img2018.cnblogs.com/blog/1247221/201903/1247221-20190321205138145-77073147.png)

```
1）下载tez的依赖包：http://tez.apache.org
2）拷贝apache-tez-0.9.1-bin.tar.gz到hadoop102的/opt/module目录
3）解压缩apache-tez-0.9.1-bin.tar.gz
　　[kris@hadoop101 module]$ tar -zxvf apache-tez-0.9.1-bin.tar.gz
4）修改名称
[kris@hadoop101 module]$ mv apache-tez-0.9.1-bin/ tez-0.9.1
```

```
1）进入到Hive的配置目录：/opt/module/hive/conf

2）在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置
[kris@hadoop101 conf]$ vim hive-env.sh
添加如下配置
```

```
# Set HADOOP_HOME to point to a specific hadoop install directory
export HADOOP_HOME=/opt/module/hadoop-2.7.2

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/module/hive/conf

# Folder containing extra libraries required for hive compilation/execution can be controlled by:
export TEZ_HOME=/opt/module/tez-0.9.1    #是你的tez的解压目录
export TEZ_JARS=""
for jar in `ls $TEZ_HOME |grep jar`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
done
for jar in `ls $TEZ_HOME/lib`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
done

export HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS
```



3）在hive-site.xml文件中添加如下配置，更改hive计算引擎

```
<property>
  <name>hive.execution.engine</name>
  <value>tez</value>
</property>
```

**配置Tez**

```
1）在Hive 的/opt/module/hive/conf下面创建一个tez-site.xml文件
[kris@hadoop101 conf]$ pwd
/opt/module/hive/conf
[kris@hadoop101 conf]$ vim tez-site.xml
添加如下内容
```

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
    <name>tez.lib.uris</name>    <value>${fs.defaultFS}/tez/tez-0.9.1,${fs.defaultFS}/tez/tez-0.9.1/lib</value>
</property>
<property>
    <name>tez.lib.uris.classpath</name>        <value>${fs.defaultFS}/tez/tez-0.9.1,${fs.defaultFS}/tez/tez-0.9.1/lib</value>
</property>
<property>
     <name>tez.use.cluster.hadoop-libs</name>
     <value>true</value>
</property>
<property>
     <name>tez.history.logging.service.class</name>        <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
</property>
</configuration>
```



**上传Tez到集群**

```
1）将/opt/module/tez-0.9.1上传到HDFS的/tez路径
[kris@hadoop101 conf]$ hadoop fs -mkdir /tez
[kris@hadoop101 conf]$ hadoop fs -put /opt/module/tez-0.9.1/ /tez
[kris@hadoop101 conf]$ hadoop fs -ls /tez
/tez/tez-0.9.1
```

测试

```bash
1）启动Hive
[kris@hadoop101 hive]$ bin/hive
2）创建LZO表
hive (default)> create table student(
id int,
name string);
3）向表中插入数据
hive (default)> insert into student values(1,"zhangsan");
4）如果没有报错就表示成功了
hive (default)> select * from student;
1       zhangsan
```

小结
1）运行Tez时检查到用过多内存而被NodeManager杀死进程问题：

这种问题是从机上运行的Container试图使用过多的内存，而被NodeManager kill掉了。

**解决方法：**

方案一：或者是关掉虚拟内存检查。我们选这个，修改yarn-site.xml；**修改完之后要分发**

```
<property>
     <name>yarn.nodemanager.vmem-check-enabled</name>
     <value>false</value>
</property>
```

方案二：mapred-site.xml中设置Map和Reduce任务的内存配置如下：(value中实际配置的内存需要根据自己机器内存大小及应用情况进行修改)

```xml
<property>
　　<name>mapreduce.map.memory.mb</name>
　　<value>1536</value>
</property>
<property>
　　<name>mapreduce.map.java.opts</name>
　　<value>-Xmx1024M</value>
</property>
<property>
　　<name>mapreduce.reduce.memory.mb</name>
　　<value>3072</value>
</property>
<property>
　　<name>mapreduce.reduce.java.opts</name>
　　<value>-Xmx2560M</value>
</property>
```

**② Hive on Spark配置 Hive3.1.2、Spark3.0.0、hadoop3.x** 

1）兼容性说明

注意：官网下载的Hive3.1.2和Spark3.0.0默认是不兼容的。因为Hive3.1.2支持的Spark版本是2.4.5，所以重新编译Hive3.1.2版本。

编译步骤：官网下载Hive3.1.2源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，就根据提示，修改相关方法，直到不报错，打包获取jar包。

2）在Hive所在节点部署Spark

```
如果之前已经部署了Spark，则该步骤可以跳过，但要检查SPARK_HOME的环境变量配置是否正确。

（1）Spark官网下载jar包地址： http://spark.apache.org/downloads.html

（2）上传并解压解压spark-3.0.0-bin-hadoop3.2.tgz
[kris@hadoop101 software]$ tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/ 
[kris@hadoop101 software]$ mv /opt/module/spark-3.0.0-bin-hadoop3.2 /opt/module/spark
（3）配置SPARK_HOME环境变量
　　[kris@hadoop101 software]$ sudo vim /etc/profile

# SPARK_HOME
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin
[kris@hadoop101 software]$ source /etc/profile

（4）新建spark配置文件

[kris@hadoop101 software]$ vim /opt/module/hive/conf/spark-defaults.conf   添加如下内容（在执行任务时，会根据如下参数执行）

spark.master yarn
spark.eventLog.enabled true
spark.eventLog.dir hdfs://hadoop101:8020/spark-history
spark.executor.memory 1g
spark.driver.memory    1g
（5）在HDFS创建如下路径，用于存储历史日志

[kris@hadoop101 software]$ hadoop fs -mkdir /spark-history
```

3）向HDFS上传Spark纯净版jar包

```
说明1：由于Spark3.0.0非纯净版默认支持的是hive2.3.7版本，直接使用会和安装的Hive3.1.2出现兼容性问题。所以采用Spark纯净版jar包，不包含hadoop和hive相关依赖，避免冲突。
说明2：Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。

（1）上传并解压spark-3.0.0-bin-without-hadoop.tgz
[kris@hadoop101 software]$ tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz

（2）上传Spark纯净版jar包到HDFS
[kris@hadoop101 software]$ hadoop fs -mkdir /spark-jars

[kris@hadoop101 software]$ hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
```

4）修改hive-site.xml文件

[kris@hadoop101 ~]$ vim /opt/module/hive/conf/hive-site.xml

```xml
<!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
<property>
<name>spark.yarn.jars</name>
<value>hdfs://hadoop101:8020/spark-jars/*</value>
</property>

<!--Hive执行引擎-->
<property>
<name>hive.execution.engine</name>
<value>spark</value>
</property>

<!--Hive和Spark连接超时时间-->
<property>
<name>hive.spark.client.connect.timeout</name>
<value>10000ms</value>
</property>
```

注意：hive.spark.client.connect.timeout的默认值是1000ms，如果执行hive的insert语句时，抛如下异常，可以调大该参数到10000ms

FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create Spark client for Spark session d9e0224c-3d14-4bf4-95bc-ee3ec56df48e

**Hive on Spark测试**

（1）启动hive客户端

（2）创建一张测试表
　　　　hive (default)> create table student(id int, name string);
（3）通过insert测试效果

　　hive (default)> insert into table student values(1,'abc');

若结果如下，则说明配置成功

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201209141224397-50120417.png)

### Yarn容量调度器并发度问题

**① 增加ApplicationMaster资源比例**

针对容量调度器并发度低的问题，考虑调整yarn.scheduler.capacity.maximum-am-resource-percent该参数。默认值是0.1，表示集群上AM最多可使用的资源比例，目的为限制过多的app数量。

（1）在hadoop101的/opt/module/hadoop-3.1.3/etc/Hadoop/capacity-scheduler.xml文件中修改如下参数值

```
[kris@hadoop101 hadoop]$ vim capacity-scheduler.xml

<property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.5</value>
    <description>
      集群中用于运行应用程序ApplicationMaster的资源比例上限，
    该参数通常用于限制处于活动状态的应用程序数目。该参数类型为浮点型，
    默认是0.1，表示10%。所有队列的ApplicationMaster资源比例上限可通过参数
    yarn.scheduler.capacity.maximum-am-resource-percent设置，而单个队列可通过参数yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent设置适合自己的值。
    </description>
</property
```

（2）分发capacity-scheduler.xml配置文件

　　[kris@hadoop101 hadoop]$ xsync capacity-scheduler.xml

（3）关闭正在运行的任务，重新启动yarn集群

**② 增加Yarn容量调度器队列**

方案二：创建多队列，也可以增加容量调度器的并发度。

在企业里面如何配置多队列：

**按照计算引擎创建队列hive、spark、flink**

**按照业务创建队列：下单、支付、点赞、评论、收藏（用户、活动、优惠相关）**

好处：

假如公司来了一个菜鸟，写了一个递归死循环，公司集群资源耗尽，大数据全部瘫痪。解耦。

假如11.11数据量非常大，任务非常多，如果所有任务都参与运行，一定执行不完，怎么办？ 可以支持降级运行。

下单 √

　　支付√

　　　　点赞X

1）增加容量调度器队列

（1）修改容量调度器配置文件

默认Yarn的配置下，容量调度器只有一条default队列。在capacity-scheduler.xml中可以配置多条队列，修改以下属性，增加hive队列。

```xml
<property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,hive</value>
    <description>
     再增加一个hive队列
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>50</value>
    <description>
      default队列的容量为50%
    </description>
</property>


<property>
    <name>yarn.scheduler.capacity.root.hive.capacity</name>
    <value>50</value>
    <description>
      hive队列的容量为50%
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
    <value>1</value>
    <description>
      一个用户最多能够获取该队列资源容量的比例，取值0-1
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
    <value>80</value>
    <description>
      hive队列的最大容量（自己队列资源不够，可以使用其他队列资源上限）
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.state</name>
    <value>RUNNING</value>
    <description>
      开启hive队列运行，不设置队列不能使用
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
    <value>*</value>
    <description>
      访问控制，控制谁可以将任务提交到该队列,*表示任何人
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
    <value>*</value>
    <description>
      访问控制，控制谁可以管理(包括提交和取消)该队列的任务，*表示任何人
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
    <value>*</value>
    <description>
      指定哪个用户可以提交配置任务优先级
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
    <value>-1</value>
    <description>
      hive队列中任务的最大生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。
    </description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
    <value>-1</value>
    <description>
      hive队列中任务的默认生命时长，以秒为单位。任何小于或等于零的值将被视为禁用。
    </description>
</property>
```

（2）分发配置文件

[kris@hadoop101 ~]$ xsync /opt/module/hadoop-3.1.3/etc/hadoop/capacity-scheduler.xml

（3）重启Hadoop集群

2）测试新队列

（1）提交一个MR任务，并指定队列为hive

[kris@hadoop101 ~]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi -Dmapreduce.job.queuename=hive 1 1

（2）查看ResourceManager的web页面，观察任务被提交到的队列

## Sqoop安装

详细见 https://www.cnblogs.com/shengyang17/p/10512510.html

验证sqoop配置是否正确：

[kris@hadoop101 sqoop]$ **bin/sqoop help**

测试Sqoop是否能够成功连接数据库

[kris@hadoop101 sqoop]$ **bin/sqoop list-databases --connect jdbc:mysql://hadoop101:3306/ --username root --password 000000**

出现如下输出：
　　information_schema
　　metastore
　　mysql
　　oozie
　　performance_schema

# 4.同步策略

数据同步策略的类型包括：全量表、增量表、新增及变化表、特殊表、拉链表 

- 全量表：存储完整的数据。
- 增量表：存储新增加的数据。
- 新增及变化表：存储新增加的数据和变化的数据。
- 特殊表：只需要存储一次。

- 拉链表：对新增及变化表**做定期合并**。 **利用每日新增和变化表，制作一张拉链表，以方便的取到某个时间切片的快照数据**

**① 全量同步策略**

​    **![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201206203347951-1226238390.png)**

**② 增量同步策略**

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201206203457511-771570255.png)

**③ 新增及变化同步策略**

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201206203554712-1134431876.png)

**④ 特殊策略** 

某些特殊的维度表，可不必遵循上述同步策略。

- **1****）客观世界维度**

　　　　没变化的客观世界的维度（比如性别，地区，民族，政治成分，鞋子尺码）可以只存一份固定值。

- **2****）日期维度**

　　　　日期维度可以一次性导入一年或若干年的数据。

- **3****）地区维度**

　　　　省份表、地区表

**⑤ 分析表同步策略**

 ![img](https://img2020.cnblogs.com/blog/1247221/202012/1247221-20201206204552008-515538607.png)

 **同步脚本的编写：**

在/home/kris/bin目录下创建 [kris@hadoop101 bin]$ **vim mysql_to_hdfs.sh**

```bash
#! /bin/bash

sqoop=/opt/module/sqoop/bin/sqoop

if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d '-1 day' +%F`
fi

import_data(){
$sqoop import \
--connect jdbc:mysql://hadoop101:3306/gmall \
--username root \
--password 000000 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \
--query "$2 and  \$CONDITIONS" \
--num-mappers 1 \
--fields-terminated-by '\t' \
--compress \
--compression-codec lzop \
--null-string '\\N' \
--null-non-string '\\N'

hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/gmall/db/$1/$do_date
}

import_order_info(){
  import_data order_info "select
                            id, 
                            final_total_amount, 
                            order_status, 
                            user_id, 
                            out_trade_no, 
                            create_time, 
                            operate_time,
                            province_id,
                            benefit_reduce_amount,
                            original_total_amount,
                            feight_fee      
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date' 
                        or date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_coupon_use(){
  import_data coupon_use "select
                          id,
                          coupon_id,
                          user_id,
                          order_id,
                          coupon_status,
                          get_time,
                          using_time,
                          used_time
                        from coupon_use
                        where (date_format(get_time,'%Y-%m-%d')='$do_date'
                        or date_format(using_time,'%Y-%m-%d')='$do_date'
                        or date_format(used_time,'%Y-%m-%d')='$do_date')"
}

import_order_status_log(){
  import_data order_status_log "select
                                  id,
                                  order_id,
                                  order_status,
                                  operate_time
                                from order_status_log
                                where date_format(operate_time,'%Y-%m-%d')='$do_date'"
}

import_activity_order(){
  import_data activity_order "select
                                id,
                                activity_id,
                                order_id,
                                create_time
                              from activity_order
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_user_info(){
  import_data "user_info" "select 
                            id,
                            name,
                            birthday,
                            gender,
                            email,
                            user_level, 
                            create_time,
                            operate_time
                          from user_info 
                          where (DATE_FORMAT(create_time,'%Y-%m-%d')='$do_date' 
                          or DATE_FORMAT(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
  import_data order_detail "select 
                              od.id,
                              order_id, 
                              user_id, 
                              sku_id,
                              sku_name,
                              order_price,
                              sku_num, 
                              od.create_time,
                              source_type,
                              source_id  
                            from order_detail od
                            join order_info oi
                            on od.order_id=oi.id
                            where DATE_FORMAT(od.create_time,'%Y-%m-%d')='$do_date'"
}

import_payment_info(){
  import_data "payment_info"  "select 
                                id,  
                                out_trade_no, 
                                order_id, 
                                user_id, 
                                alipay_trade_no, 
                                total_amount,  
                                subject, 
                                payment_type, 
                                payment_time 
                              from payment_info 
                              where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
}

import_comment_info(){
  import_data comment_info "select
                              id,
                              user_id,
                              sku_id,
                              spu_id,
                              order_id,
                              appraise,
                              comment_txt,
                              create_time
                            from comment_info
                            where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_order_refund_info(){
  import_data order_refund_info "select
                                id,
                                user_id,
                                order_id,
                                sku_id,
                                refund_type,
                                refund_num,
                                refund_amount,
                                refund_reason_type,
                                create_time
                              from order_refund_info
                              where date_format(create_time,'%Y-%m-%d')='$do_date'"
}

import_sku_info(){
  import_data sku_info "select 
                          id,
                          spu_id,
                          price,
                          sku_name,
                          sku_desc,
                          weight,
                          tm_id,
                          category3_id,
                          create_time
                        from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
                                  id,
                                  name 
                                from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select
                                  id,
                                  name,
                                  category1_id 
                                from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select
                                  id,
                                  name,
                                  category2_id
                                from base_category3 where 1=1"
}

import_base_province(){
  import_data base_province "select
                              id,
                              name,
                              region_id,
                              area_code,
                              iso_code
                            from base_province
                            where 1=1"
}

import_base_region(){
  import_data base_region "select
                              id,
                              region_name
                            from base_region
                            where 1=1"
}

import_base_trademark(){
  import_data base_trademark "select
                                tm_id,
                                tm_name
                              from base_trademark
                              where 1=1"
}

import_spu_info(){
  import_data spu_info "select
                            id,
                            spu_name,
                            category3_id,
                            tm_id
                          from spu_info
                          where 1=1"
}

import_favor_info(){
  import_data favor_info "select
                          id,
                          user_id,
                          sku_id,
                          spu_id,
                          is_cancel,
                          create_time,
                          cancel_time
                        from favor_info
                        where 1=1"
}

import_cart_info(){
  import_data cart_info "select
                        id,
                        user_id,
                        sku_id,
                        cart_price,
                        sku_num,
                        sku_name,
                        create_time,
                        operate_time,
                        is_ordered,
                        order_time,
                        source_type,
                        source_id
                      from cart_info
                      where 1=1"
}

import_coupon_info(){
  import_data coupon_info "select
                          id,
                          coupon_name,
                          coupon_type,
                          condition_amount,
                          condition_num,
                          activity_id,
                          benefit_amount,
                          benefit_discount,
                          create_time,
                          range_type,
                          spu_id,
                          tm_id,
                          category3_id,
                          limit_num,
                          operate_time,
                          expire_time
                        from coupon_info
                        where 1=1"
}

import_activity_info(){
  import_data activity_info "select
                              id,
                              activity_name,
                              activity_type,
                              start_time,
                              end_time,
                              create_time
                            from activity_info
                            where 1=1"
}

import_activity_rule(){
    import_data activity_rule "select
                                    id,
                                    activity_id,
                                    condition_amount,
                                    condition_num,
                                    benefit_amount,
                                    benefit_discount,
                                    benefit_level
                                from activity_rule
                                where 1=1"
}

import_base_dic(){
    import_data base_dic "select
                            dic_code,
                            dic_name,
                            parent_code,
                            create_time,
                            operate_time
                          from base_dic
                          where 1=1"
}

case $1 in
  "order_info")
     import_order_info
;;
  "base_category1")
     import_base_category1
;;
  "base_category2")
     import_base_category2
;;
  "base_category3")
     import_base_category3
;;
  "order_detail")
     import_order_detail
;;
  "sku_info")
     import_sku_info
;;
  "user_info")
     import_user_info
;;
  "payment_info")
     import_payment_info
;;
  "base_province")
     import_base_province
;;
  "base_region")
     import_base_region
;;
  "base_trademark")
     import_base_trademark
;;
  "activity_info")
      import_activity_info
;;
  "activity_order")
      import_activity_order
;;
  "cart_info")
      import_cart_info
;;
  "comment_info")
      import_comment_info
;;
  "coupon_info")
      import_coupon_info
;;
  "coupon_use")
      import_coupon_use
;;
  "favor_info")
      import_favor_info
;;
  "order_refund_info")
      import_order_refund_info
;;
  "order_status_log")
      import_order_status_log
;;
  "spu_info")
      import_spu_info
;;
  "activity_rule")
      import_activity_rule
;;
  "base_dic")
      import_base_dic
;;

"first")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_province
   import_base_region
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
"all")
   import_base_category1
   import_base_category2
   import_base_category3
   import_order_info
   import_order_detail
   import_sku_info
   import_user_info
   import_payment_info
   import_base_trademark
   import_activity_info
   import_activity_order
   import_cart_info
   import_comment_info
   import_coupon_use
   import_coupon_info
   import_favor_info
   import_order_refund_info
   import_order_status_log
   import_spu_info
   import_activity_rule
   import_base_dic
;;
esac
```

说明1：

[ -n 变量值 ] 判断变量的值，是否为空

-- 变量的值，非空，返回true

-- 变量的值，为空，返回false

说明2：

查看date命令的使用

初次导入 [kris@hadoop101 bin]$ mysql_to_hdfs.sh first 2020-06-14

每日导入 [kris@hadoop101 bin]$ mysql_to_hdfs.sh all 2020-06-15

```
Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。
在导出数据时采用--input-null-string和--input-null-non-string两个参数。
导入数据时采用--null-string和--null-non-string。
```