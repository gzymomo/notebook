- [实时数仓方案 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/315437549)
- [实时数据仓库的演进_浊酒南街的博客-CSDN博客](https://blog.csdn.net/weixin_43597208/article/details/125794590)

## 1、建设目的

解决离线数仓时效性低解决不了的问题

## 2、应用场景

￮ 实时数据看板

￮ 实时olap分析

￮ 实时业务监控

￮ 实时数据接口服务

## 3、架构图

### 3.1 实时数仓常见架构

#### 3.1.1 Lambda架构

![img](https://pic2.zhimg.com/80/v2-0193ac43570047b61eedbc97cf6426fd_720w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

#### 3.1.2 Kappa架构

![img](https://pic4.zhimg.com/80/v2-2452a10488834e857f622de787054d4b_1440w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

#### 3.1.3 实时olap变体架构

![img](https://pic2.zhimg.com/80/v2-62e44a7def394801cfe1a97cd4e58191_1440w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

#### 3.1.4 常见架构对比

![img](https://pic4.zhimg.com/80/v2-0c1f236774f4f1aa9feb070cbea50acf_1440w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

ps:lambda架构

开发割裂感：

• 表结构不同

• sql语法不同

资源浪费：

• 重复计算

• 重复存储

集群维护：

• 组件不同

• 计算引擎不同

数据一致性

### 3.2 实时数仓架构

![img](https://pic3.zhimg.com/80/v2-6f221f5c6cfc229cd60c8adeab15da72_1440w.webp)

#### 3.2.1 方案一

![img](https://pic3.zhimg.com/80/v2-92ffe2a9018446967c17bf320b8430e6_1440w.webp)

优点：

￮ 便于数据回溯、重算和数据质量验证。

缺点：

￮ 通过批处理重算，需要维护两套代码，开发和维护成本高。

￮ 需要两套计算资源

适用场景：

￮ 超大规模历史数据计算，且这种场景比较频繁。

￮ 对数据质量要求极高，需要比对实时和离线的计算结果，甚至利用离线去修正实时的计算结果。

#### 3.2.2 方案二

![img](https://pic4.zhimg.com/80/v2-a9f9e16d04a7833d8ae60f6aa2853cef_1440w.webp)

优点：

￮ 无需维护两套代码，开发迭代速度快。

￮ 数据回溯和重算方便，重算时间根据需求回溯的时间范围定。

￮ 只需流计算资源，资源占用小

缺点：

￮ ODS\DWD部分数据“不可见”，原始数据和中间数据不便于查询（解决方案：可通过重新消费指定时间范围的数据查询，或导入需要的数据到olap引擎）

￮ 依赖业务端反馈问题（解决方案：设计数据质量监控指标，实时监控报警）

适用场景：

ODS\DWD查询不频繁等

#### 3.2.3 方案三

![img](https://pic3.zhimg.com/80/v2-2ad2f93e577af69d51e978d425a191c2_1440w.webp)

相对于方案二：

￮ 增加ODS层落地hive，排查分析原始数据比较方便，恢复历史数据的时候可获取hive数据写入kafka，然后按原流处理的逻辑重新处理即可，只需修改数据源为历史数据对应的topic。

￮ 需新增kafka写入hive逻辑

￮ 需新增从hive读取数据写入kafka

￮ 需新增整条链路历史数据对应的topic

## 4、实时数仓分层

### 4.1 ODS源数据层

#### 4.1.1 数据来源

i. 服务器打过来的日志（新老日志接口）

ii. 第三方实时回传（Appsflyer Push）

iii. 流计算处理回流

iv. 拉取的第三方数据（Appsflyer、花费、Facebook素材）

#### 4.1.2 注意事项

▪ 数据来源尽可能统一

▪ 利用分区保证数据局部有序

### 4.2 DWD明细层

#### 4.2.1 数据来源

ODS层数据

#### 4.2.2 建设实践

▪ 维度字段冗余

▪ 宽表化处理

▪ 简单数据清洗

▪ 对ODS进行stream join

▪ 实时写入hive\hbase\clickhouse

#### 4.2.3 注意事项

▪ 模型规范化，成功了一半

▪ 每条数据额外补充一些信息，解决实时数据生产环节的常见问题

![img](https://pic4.zhimg.com/80/v2-8cc8ee5c3668752ab2ea012550bdf1e3_1440w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

### 4.3 DIM维度层

#### 4.3.1 数据来源

▪ 实时处理数据得到

▪ 离线计算得到

#### 4.3.2 建设实践

￮ 变化频率高的维度

将维表数据同步到缓存中，或实时写入hbase

￮ 变化频率低的维度

存入关系型数据库

通过维度数据变化的消息构建拉链表

通过事实数据计算衍生维度构建拉链表

应用：通过UDTF关联

### 4.4 DWS汇总层

#### 4.4.1 数据来源

DWD\DIM层数据

#### 4.4.2 建设实践

▪ 对一些共性指标的加工

▪ 面向主题进行汇总，最后算业务方需要的汇总指标

▪ 衍生维度指标使用hbase的版本机制构建衍生维度的拉链表

▪ flink丰富的时间窗口（state ttl设置）

## 5、实时数仓与离线数仓的区别

1.相关概念区别

![img](https://pic1.zhimg.com/80/v2-0db813961b976f57e60f856a02e441e4_1440w.webp)

来自Apache Flink 中文学习网站 ververica.cn 侵权告知立删

2.层次更少

多一层必然增加延迟，部分聚合场景有人为的延迟

应用层直接写入应用数据库，数仓不维护应用层

3.数据源存储不同

离线存hive

实时存kafka\hbase\clickhouse\redis等

## 6、数据质量管理

### 6.1 监控指标

实时数据写入hive，使用离线数据持续验证实时数据的准确性。

在实时数仓中，缺乏有效的数据质量监控管理，将会导致脏数据、冗余数据、不一致数据的产生，从而会引起数据无法整合、可用性差、计算性能低下等问题。对实时数仓而言，数据质量的评估维度主要是指完整性、一致性、实时性、准确性。

| 数据质量维度 | 描述                                           |
| ------------ | ---------------------------------------------- |
| 完整性       | 业务需求的数据集完备                           |
| 一致性       | 数据流转中，数据的逻辑意义保持一致，不存在冲突 |
| 实时性       | 数据全生命周期流转的实时性                     |
| 准确性       | 可以表达真实准确的业务指标                     |

监控指标示例：

￮ 消费偏移量监控

￮ 脏数据计数监控

￮ 格式异常计数监控

￮ 实时离线差异监控

￮ 中间状态监控

￮ 同比环比监控

￮ 等等

### 6.2 元数据中心

元数据中心的好处：

可以通过元数据中心快速找到自己想要的数据，了解数据表的连接信息、schema 信息，字段的业务含义，以及所有表的数据来源和走向。

代码里面不用维护表元数据信息。

目前flink与hive的元数据已经打通。

## 7.基于Flink实现典型的ETL场景

7.1 维表join

7.1.1 预加载维表

7.1.2 热存储关联

7.1.3 广播维表

7.1.4 temporal table function join

7.1.5 方案对比

7.2 双流join

7.2.1 regular join

7.2.2 interal join

7.2.3 window join